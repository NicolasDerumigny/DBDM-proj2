#!/usr/bin/env  python3
#import sklearn as sk
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
#import tweepy
#from tweepy import OAuthHandler
import json
import sys
from datetime import datetime
from itertools import combinations
import apriori as ap




NUMBER_CANDIDATES=11

id_to_candidates=["JLMelenchon",
	"PhilippePoutou",
	"FrancoisFillon",
	"JCheminade",
	"benoithamon",
	"n_arthaud",
	"MLP_officiel",
	"EmmanuelMacron",
	"jeanlassalle",
	"UPR_Asselineau",
	"dupontaignan"]

id_to_candidates_pretty=["Melenchon",
	"Poutou",
	"Fillon",
	"Cheminade",
	"Hamon",
	"Arthaud",
	"Le Pen",
	"Macron",
	"Lassalle",
	"Asselineau",
	"Dupont-Aignan"]

candidate_to_id={"JLMelenchon":0,
	"PhilippePoutou": 1,
	"FrancoisFillon": 2,
	"JCheminade": 3,
	"benoithamon": 4,
	"n_arthaud": 5,
	"MLP_officiel": 6,
	"EmmanuelMacron": 7,
	"jeanlassalle": 8,
	"UPR_Asselineau": 9,
	"dupontaignan": 10}

data=dict()

def collect_data():
	consumer_key = 'neYds1jpNT2b9HKqd80A7fsXi'
	consumer_secret = 'd7loWWSA3EV8yIjfP18VAL7eysH7eCvZNAGzIom1dBif85vdpS'
	access_token = '848790876-RnPsJCHGSqAx4qeY3lYp92q3dimjrWXidb5vhmr7'
	access_secret = 'YSjVnF5R18IOO0wmYrEtTrVu2MzkuUg9nDdaMl0qkge2H'
	 
	auth = OAuthHandler(consumer_key, consumer_secret)
	auth.set_access_token(access_token, access_secret)
	 
	api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
	
	for candidate in id_to_candidates:
		 file=open(candidate+".dat","w+")
		 data[candidate]=api.user_timeline(candidate)
		 print(type(api.user_timeline(candidate)))
		 file.write(str(data[candidate]))
		 file.close()


def main():
	load_data()
	#mining_from_time_partition(["Jan 1 00:00:00 +0000 2012", "Jan 30 00:00:00 +0000 2017", "May 07 00:00:00 +0000 2017"])

	#mining_from_time_partition(["Jan 30 00:00:00 +0000 2017", "Mar 01 00:00:00 +0000 2017", "May 07 00:00:00 +0000 2017"])

	mining_from_time_partition(["Jan 30 00:00:00 +0000 2017", "May 07 00:00:00 +0000 2017"])
	"""freq_words=[dict() for i in id_to_candidates]
	nb_words=[0 for i in id_to_candidates]
	hashtag=[dict() for i in id_to_candidates]
	nb_hash=[0 for i in id_to_candidates]
	lst_tweet=[]
	load_num_freq_candidate("Jan 1 00:00:00 +0000 2012", "May 07 00:00:00 +0000 2017", freq_words, nb_words, hashtag, nb_hash, lst_tweet=[])
	dst, dst_hash=compute_distances_all(freq_words, nb_words, hashtag, nb_hash)
	print_clustering(dst, dst_hash, 2)"""




def mining_from_time_partition(date):	
	for period in range(len(date)-1):
		freq_words=[dict() for i in id_to_candidates]
		nb_words=[0 for i in id_to_candidates]
		hashtag=[dict() for i in id_to_candidates]
		nb_hash=[0 for i in id_to_candidates]
		lst_tweet=[]
		print("\n\n ------- From {} to {} -------\n".format(date[period], date[period+1]))
		load_num_freq_candidate(date[period], date[period+1], freq_words, nb_words, hashtag, nb_hash, lst_tweet)
		dst=[]
		dst_hash=[]
		for i in range(NUMBER_CANDIDATES):
			items, rules=ap.runApriori(lst_tweet[i], 0.01, 0.1)
			print()
			print(id_to_candidates_pretty[i])
			ap.printResults(items, rules)
		# 	dst+=[[]]
		# 	dst_hash+=[[]]
		# for candidate1 in range(NUMBER_CANDIDATES):
		#  	for candidate2 in range(NUMBER_CANDIDATES):
		#  		dst[candidate1]+=[compute_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)]
		#  		dst_hash[candidate1]+=[compute_hashtag_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)]

		# print_clustering(dst, dst_hash, 2)

def compute_distances_all(freq_words, nb_words, hashtag, nb_hash):
	dst=[[] for i in id_to_candidates]
	dst_hash=[[] for i in id_to_candidates]
	for candidate1 in range(NUMBER_CANDIDATES):
		for candidate2 in range(NUMBER_CANDIDATES):
			dst[candidate1]+=[compute_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)]
			dst_hash[candidate1]+=[compute_hashtag_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)]

	return(dst, dst_hash)




def load_data():
	print("Loading", end="")
	sys.stdout.flush()
	for candidate in id_to_candidates:
		myfile=open("../data_proj2/" + candidate + ".json")
		print(".", end="")
		sys.stdout.flush()
		data[candidate]=json.load(myfile)
	print("\nLoaded!\n")
	return 0

def load_num_freq_candidate(time_min, time_max, freq_words, nb_words, hashtag, nb_hash, lst_tweet=[]):
	ntime_max=datetime.strptime(time_max, '%b %d %H:%M:%S %z %Y')
	ntime_min=datetime.strptime(time_min, '%b %d %H:%M:%S %z %Y')
	charged=0
	for candidate in range(NUMBER_CANDIDATES) :
		lst_cand=[]
		nb_words[candidate]=0
		num_tweet=data[id_to_candidates[candidate]]["nb"]
		for tweet_id in range(num_tweet):
			time=datetime.strptime(data[id_to_candidates[candidate]]["tweets"][tweet_id]["created_at"], '%a %b %d %H:%M:%S %z %Y')
			if time>ntime_max or time<=ntime_min:
				continue
			
			charged+=1
			tweet=data[id_to_candidates[candidate]]["tweets"][tweet_id]["full_text"]
			for carac in ";.?![],():/\"'â€™@":
				tweet=tweet.replace(carac ," ")
			words=tweet.lower().split()
			tweet_interesting_words=[]
			for word in words :
				if word[0]=='#':
					if word in hashtag[candidate]:
						hashtag[candidate][word]+=1
					else:
						hashtag[candidate][word]=1
					nb_hash[candidate]+=1

				if len(word)>4 and word not in {"notre", "votre", "https", "faire", "cette"}:
					tweet_interesting_words+=[word]
					nb_words[candidate]+=1
					if word in freq_words[candidate]:
						freq_words[candidate][word]+=1
					else:
						freq_words[candidate][word]=1

			lst_cand+=[tweet_interesting_words]
		lst_tweet+=[lst_cand]
	#print(charged)
	return 0

def most_frequent(nb, freq_words, nb_words, hashtag, nb_hash):			
	for candidate in range(NUMBER_CANDIDATES):
		sorted_words=sorted(freq_words[candidate].items(), key=lambda t: t[1])
		sorted_hash=sorted(hashtag[candidate].items(), key=lambda t: t[1])
		print("candidate {}: ".format(id_to_candidates[candidate]))
		for i in range(1,nb+1):
			print("({} word): {} with freq {}%".format(i, sorted_words[-i][0], sorted_words[-i][1]*100/nb_words[candidate]))
			print("({} hash): {} with freq {}%".format(i, sorted_hash[-i][0], sorted_hash[-i][1]*100/nb_hash[candidate]))

def compute_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash):
	# Get the proportion of words of candidate1 and cadidat2 that
	# are not in common
	sum_dif_words12=0 
	for key, data in freq_words[candidate1].items():
		if key not in freq_words[candidate2]:
			sum_dif_words12+=data
	sum_dif_words21=0
	for key, data in freq_words[candidate2].items():
		if key not in freq_words[candidate1]:
			sum_dif_words21+=data

	return (sum_dif_words21/nb_words[candidate2] + sum_dif_words12/nb_words[candidate1])/2


def compute_hashtag_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash):
	# Get the proportion of hashtags of candidate1 and cadidat2 that
	# are not in common
	sum_dif_words12=0 
	for key, data in hashtag[candidate1].items():
		if key not in hashtag[candidate2]:
			sum_dif_words12+=data
	sum_dif_words21=0
	for key, data in hashtag[candidate2].items():
		if key not in hashtag[candidate1]:
			sum_dif_words21+=data

	return (sum_dif_words21/nb_hash[candidate2] + sum_dif_words12/nb_hash[candidate1])/2

def compute_sum_distance_all(freq_words, nb_words, hashtag, nb_hash):
	dst_sum=[0 for i in id_to_candidates]
	dst_hash_sum=[0 for i in id_to_candidates]
	for candidate1 in range(NUMBER_CANDIDATES):
		for candidate2 in range(NUMBER_CANDIDATES):
			dst_sum[candidate1]+= compute_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)
			dst_hash_sum[candidate1]+=compute_hashtag_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)
	return(dst_sum, dst_hash_sum)



def show_distance(freq_words, nb_words, hashtag, nb_hash):
	x=[0]*NUMBER_CANDIDATES
	y=[0]*NUMBER_CANDIDATES
	dst_sum, dst_hash_sum = compute_sum_distance_all(freq_words, nb_words, hashtag, nb_hash)
	for i in range(NUMBER_CANDIDATES):
		x[i]=dst_sum[i]
		y[i]=dst_hash_sum[i]
	plt.scatter(x,y, color='blue')
	for i in range(NUMBER_CANDIDATES):
		plt.annotate(
			id_to_candidates_pretty[i],
			xy=(x[i], y[i]), xytext=(-5, 5),
			textcoords='offset points', ha='right', va='bottom',
			bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),
			arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))
		plt.ylabel('Hashtag distance')
		plt.xlabel("Word distance")
	plt.show()


def print_clustering(dst, dst_hash, nbclusters):
	print("kMeans:")
	cluster = KMeans(n_clusters=nbclusters, random_state=0).fit(dst)
	cluster_hash = KMeans(n_clusters=nbclusters, random_state=0).fit(dst_hash)
	print("Words:")
	for i in range(nbclusters):
		print("Class {}:".format(i), end="")
		for j in range(NUMBER_CANDIDATES):
			if cluster.labels_[j]==i:
				print(" "+id_to_candidates_pretty[j], end="")
		print()

	print("\nHashes:")
	for i in range(nbclusters):
		print("Class {}:".format(i), end="")
		for j in range(NUMBER_CANDIDATES):
			if cluster_hash.labels_[j]==i:
				print(" "+id_to_candidates_pretty[j], end="")
		print()

	print("\n\nHierarchical clustering:")
	cluster = AgglomerativeClustering(n_clusters=nbclusters).fit(dst)
	cluster_hash = AgglomerativeClustering(n_clusters=nbclusters).fit(dst_hash)
	print("Words:")
	for i in range(nbclusters):
		print("Class {}:".format(i), end="")
		for j in range(NUMBER_CANDIDATES):
			if cluster.labels_[j]==i:
				print(" "+id_to_candidates_pretty[j], end="")
		print()

	print("\nHashes:")
	for i in range(nbclusters):
		print("Class {}:".format(i), end="")
		for j in range(NUMBER_CANDIDATES):
			if cluster_hash.labels_[j]==i:
				print(" "+id_to_candidates_pretty[j], end="")
		print()




if __name__=="__main__":
	#collect_data()
	main()
