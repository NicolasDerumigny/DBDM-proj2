#!/usr/bin/env  python3
#import sklearn as sk
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.feature_extraction.text import HashingVectorizer
#import tweepy
#from tweepy import OAuthHandler
import json
import sys
from datetime import datetime
from itertools import combinations
import apriori as ap
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import FeatureHasher




NUMBER_CANDIDATES=11

id_to_candidates=["JLMelenchon",
	"PhilippePoutou",
	"FrancoisFillon",
	"JCheminade",
	"benoithamon",
	"n_arthaud",
	"MLP_officiel",
	"EmmanuelMacron",
	"jeanlassalle",
	"UPR_Asselineau",
	"dupontaignan"]

id_to_candidates_pretty=["Melenchon",
	"Poutou",
	"Fillon",
	"Cheminade",
	"Hamon",
	"Arthaud",
	"Le Pen",
	"Macron",
	"Lassalle",
	"Asselineau",
	"Dupont-Aignan"]

candidate_to_id={"JLMelenchon":0,
	"PhilippePoutou": 1,
	"FrancoisFillon": 2,
	"JCheminade": 3,
	"benoithamon": 4,
	"n_arthaud": 5,
	"MLP_officiel": 6,
	"EmmanuelMacron": 7,
	"jeanlassalle": 8,
	"UPR_Asselineau": 9,
	"dupontaignan": 10}

stopwords=["a", "à", "alors", "au", "aucuns", "aussi", "autre", "avant", "avec", "avoir", "bon", "car", "ce", "cela", "ces", "ceux", "chaque", "ci", "comme", "comment", "dans", "des", "du", "dedans", "dehors", "depuis", "devrait", "doit", "donc", "dos", "début", "elle", "elles", "en", "encore", "essai", "est", "et", "eu", "fait", "faites", "fois", "font", "hors", "ici", "il", "ils", "je	juste", "la", "le", "les", "leur", "là", "ma", "maintenant", "mais", "mes", "mine", "moins", "mon", "mot", "même", "ni", "nommés", "notre", "nous", "ou", "où", "par", "parce", "pas", "peut", "peu", "plupart", "pour", "pourquoi", "quand", "que", "quel", "quelle", "quelles", "quels", "qui", "sa", "sans", "ses", "seulement", "si", "sien", "son", "sont", "sous", "soyez	sujet", "sur", "t", "ta", "tandis", "tellement", "tels", "tes", "ton", "tous", "tout", "trop", "très", "tu", "voient", "vont", "votre", "vous", "vu", "ça", "étaient", "état", "étions", "été", "être", "http", "https", "co", "d", "de", "n", "qu", "un", "une", "se", "suis", "rt", "on", "ne", "l", "je", "ai", "j", "via"]

data=dict()

def collect_data():
	consumer_key = 'neYds1jpNT2b9HKqd80A7fsXi'
	consumer_secret = 'd7loWWSA3EV8yIjfP18VAL7eysH7eCvZNAGzIom1dBif85vdpS'
	access_token = '848790876-RnPsJCHGSqAx4qeY3lYp92q3dimjrWXidb5vhmr7'
	access_secret = 'YSjVnF5R18IOO0wmYrEtTrVu2MzkuUg9nDdaMl0qkge2H'
	 
	auth = OAuthHandler(consumer_key, consumer_secret)
	auth.set_access_token(access_token, access_secret)
	 
	api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
	
	for candidate in id_to_candidates:
		 file=open(candidate+".dat","w+")
		 data[candidate]=api.user_timeline(candidate)
		 print(type(api.user_timeline(candidate)))
		 file.write(str(data[candidate]))
		 file.close()


def main():
	load_data()
	files_name, corpus =create_files(2, 4)
	vectorizer = CountVectorizer(min_df=1)
	file=open(files_name[0], "r")
	corpus = file.read()
	partofspeach=corpus[0]
	raw_X = (token_features(tok, partofspeach) for tok in corpus)
	hasher = FeatureHasher(input_type='string')
	X = hasher.transform(raw_X)
	print(X)



	
	"""X = vectorizer.fit_transform(corpus)
	analyze = vectorizer.build_analyzer()
	print(vectorizer.get_feature_names())"""

	#hv = HashingVectorizer()
	#hv.transform(files_name)
	#mining_from_time_partition(["Jan 1 00:00:00 +0000 2012", "Jan 30 00:00:00 +0000 2017", "May 07 00:00:00 +0000 2017"])

	#mining_from_time_partition(["Jan 30 00:00:00 +0000 2017", "Mar 01 00:00:00 +0000 2017", "May 07 00:00:00 +0000 2017"])

	#mining_from_time_partition(["Jan 30 00:00:00 +0000 2017", "May 07 00:00:00 +0000 2017"])
	"""freq_words=[dict() for i in id_to_candidates]
	nb_words=[0 for i in id_to_candidates]
	hashtag=[dict() for i in id_to_candidates]
	nb_hash=[0 for i in id_to_candidates]
	lst_tweet=[]
	load_num_freq_candidate("Jan 1 00:00:00 +0000 2012", "May 07 00:00:00 +0000 2017", freq_words, nb_words, hashtag, nb_hash, lst_tweet=[])
	dst, dst_hash=compute_distances_all(freq_words, nb_words, hashtag, nb_hash)
	print_clustering(dst, dst_hash, 2)"""

def token_features(token, part_of_speech):
    if token.isdigit():
        yield "numeric"
    else:
        yield "token={}".format(token.lower())
        yield "token,pos={},{}".format(token, part_of_speech)
    if token[0].isupper():
        yield "uppercase_initial"
    if token.isupper():
        yield "all_uppercase"
    yield "pos={}".format(part_of_speech)



def mining_from_time_partition(date):	
	for period in range(len(date)-1):
		freq_words=[dict() for i in id_to_candidates]
		nb_words=[0 for i in id_to_candidates]
		hashtag=[dict() for i in id_to_candidates]
		nb_hash=[0 for i in id_to_candidates]
		lst_tweet=[]
		print("\n\n ------- From {} to {} -------\n".format(date[period], date[period+1]))
		load_num_freq_candidate(date[period], date[period+1], freq_words, nb_words, hashtag, nb_hash, lst_tweet)
		dst=[]
		dst_hash=[]
		for i in range(NUMBER_CANDIDATES):
			items, rules=ap.runApriori(lst_tweet[i], 0.01, 0.1)
			print()
			print(id_to_candidates_pretty[i])
			ap.printResults(items, rules)
		# 	dst+=[[]]
		# 	dst_hash+=[[]]
		# for candidate1 in range(NUMBER_CANDIDATES):
		#  	for candidate2 in range(NUMBER_CANDIDATES):
		#  		dst[candidate1]+=[compute_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)]
		#  		dst_hash[candidate1]+=[compute_hashtag_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)]

		# print_clustering(dst, dst_hash, 2)

def compute_distances_all(freq_words, nb_words, hashtag, nb_hash):
	dst=[[] for i in id_to_candidates]
	dst_hash=[[] for i in id_to_candidates]
	for candidate1 in range(NUMBER_CANDIDATES):
		for candidate2 in range(NUMBER_CANDIDATES):
			dst[candidate1]+=[compute_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)]
			dst_hash[candidate1]+=[compute_hashtag_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)]

	return(dst, dst_hash)




def load_data():
	print("Loading", end="")
	sys.stdout.flush()
	for candidate in id_to_candidates:
		myfile=open("../data_proj2/" + candidate + ".json")
		print(".", end="")
		sys.stdout.flush()
		data[candidate]=json.load(myfile)
	return 0


def create_files(num_candidate, number_tweet):
	files_name=[]
	corpus=[]
	nb_word=0
	for candidate in range(num_candidate) :
		num_tweet=data[id_to_candidates[candidate]]["nb"]
		for tweet_id in range(min(number_tweet, num_tweet)):
			fichier = open(str(candidate)+"-"+str(tweet_id), "a")
			files_name += [str(candidate)+"-"+str(tweet_id)]
			tweet=data[id_to_candidates[candidate]]["tweets"][tweet_id]["full_text"]
			for carac in ";.?![],():/\"'’@":
				tweet=tweet.replace(carac ," ")
			words=tweet.lower().split()
			for word in words :
				if word not in stopwords:
					fichier.write(word)
					nb_word+=1
					fichier.write(" ")
			fichier.close()
			fichier = open(str(candidate)+"-"+str(tweet_id), "r")
			corpus += [fichier.read()]
			fichier.close()
	print(nb_word)
	return(files_name, corpus)

def load_num_freq_candidate(time_min, time_max, freq_words, nb_words, hashtag, nb_hash, lst_tweet=[]):
	ntime_max=datetime.strptime(time_max, '%b %d %H:%M:%S %z %Y')
	ntime_min=datetime.strptime(time_min, '%b %d %H:%M:%S %z %Y')
	charged=0
	for candidate in range(NUMBER_CANDIDATES) :
		lst_cand=[]
		nb_words[candidate]=0
		num_tweet=data[id_to_candidates[candidate]]["nb"]
		for tweet_id in range(num_tweet):
			time=datetime.strptime(data[id_to_candidates[candidate]]["tweets"][tweet_id]["created_at"], '%a %b %d %H:%M:%S %z %Y')
			if time>ntime_max or time<=ntime_min:
				continue
			
			charged+=1
			tweet=data[id_to_candidates[candidate]]["tweets"][tweet_id]["full_text"]
			for carac in ";.?![],():/\"'’@":
				tweet=tweet.replace(carac ," ")
			words=tweet.lower().split()
			tweet_interesting_words=[]
			for word in words :
				if word[0]=='#':
					if word in hashtag[candidate]:
						hashtag[candidate][word]+=1
					else:
						hashtag[candidate][word]=1
					nb_hash[candidate]+=1

				if word not in stopwords:
					tweet_interesting_words+=[word]
					nb_words[candidate]+=1
					if word in freq_words[candidate]:
						freq_words[candidate][word]+=1
					else:
						freq_words[candidate][word]=1

			lst_cand+=[tweet_interesting_words]
		lst_tweet+=[lst_cand]
	#print(charged)
	return 0

def most_frequent(nb, freq_words, nb_words, hashtag, nb_hash):			
	for candidate in range(NUMBER_CANDIDATES):
		sorted_words=sorted(freq_words[candidate].items(), key=lambda t: t[1])
		sorted_hash=sorted(hashtag[candidate].items(), key=lambda t: t[1])
		print("candidate {}: ".format(id_to_candidates[candidate]))
		for i in range(1,nb+1):
			print("({} word): {} with freq {}%".format(i, sorted_words[-i][0], sorted_words[-i][1]*100/nb_words[candidate]))
			print("({} hash): {} with freq {}%".format(i, sorted_hash[-i][0], sorted_hash[-i][1]*100/nb_hash[candidate]))

def compute_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash):
	# Get the proportion of words of candidate1 and cadidat2 that
	# are not in common
	sum_dif_words12=0 
	for key, data in freq_words[candidate1].items():
		if key not in freq_words[candidate2]:
			sum_dif_words12+=data
	sum_dif_words21=0
	for key, data in freq_words[candidate2].items():
		if key not in freq_words[candidate1]:
			sum_dif_words21+=data

	return (sum_dif_words21/nb_words[candidate2] + sum_dif_words12/nb_words[candidate1])/2


def compute_hashtag_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash):
	# Get the proportion of hashtags of candidate1 and cadidat2 that
	# are not in common
	sum_dif_words12=0 
	for key, data in hashtag[candidate1].items():
		if key not in hashtag[candidate2]:
			sum_dif_words12+=data
	sum_dif_words21=0
	for key, data in hashtag[candidate2].items():
		if key not in hashtag[candidate1]:
			sum_dif_words21+=data

	return (sum_dif_words21/nb_hash[candidate2] + sum_dif_words12/nb_hash[candidate1])/2

def compute_sum_distance_all(freq_words, nb_words, hashtag, nb_hash):
	dst_sum=[0 for i in id_to_candidates]
	dst_hash_sum=[0 for i in id_to_candidates]
	for candidate1 in range(NUMBER_CANDIDATES):
		for candidate2 in range(NUMBER_CANDIDATES):
			dst_sum[candidate1]+= compute_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)
			dst_hash_sum[candidate1]+=compute_hashtag_distance(candidate1, candidate2, freq_words, nb_words, hashtag, nb_hash)
	return(dst_sum, dst_hash_sum)



def show_distance(freq_words, nb_words, hashtag, nb_hash):
	x=[0]*NUMBER_CANDIDATES
	y=[0]*NUMBER_CANDIDATES
	dst_sum, dst_hash_sum = compute_sum_distance_all(freq_words, nb_words, hashtag, nb_hash)
	for i in range(NUMBER_CANDIDATES):
		x[i]=dst_sum[i]
		y[i]=dst_hash_sum[i]
	plt.scatter(x,y, color='blue')
	for i in range(NUMBER_CANDIDATES):
		plt.annotate(
			id_to_candidates_pretty[i],
			xy=(x[i], y[i]), xytext=(-5, 5),
			textcoords='offset points', ha='right', va='bottom',
			bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),
			arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))
		plt.ylabel('Hashtag distance')
		plt.xlabel("Word distance")
	plt.show()


def print_clustering(dst, dst_hash, nbclusters):
	print("kMeans:")
	cluster = KMeans(n_clusters=nbclusters, random_state=0).fit(dst)
	cluster_hash = KMeans(n_clusters=nbclusters, random_state=0).fit(dst_hash)
	print("Words:")
	for i in range(nbclusters):
		print("Class {}:".format(i), end="")
		for j in range(NUMBER_CANDIDATES):
			if cluster.labels_[j]==i:
				print(" "+id_to_candidates_pretty[j], end="")
		print()

	print("\nHashes:")
	for i in range(nbclusters):
		print("Class {}:".format(i), end="")
		for j in range(NUMBER_CANDIDATES):
			if cluster_hash.labels_[j]==i:
				print(" "+id_to_candidates_pretty[j], end="")
		print()

	print("\n\nHierarchical clustering:")
	cluster = AgglomerativeClustering(n_clusters=nbclusters).fit(dst)
	cluster_hash = AgglomerativeClustering(n_clusters=nbclusters).fit(dst_hash)
	print("Words:")
	for i in range(nbclusters):
		print("Class {}:".format(i), end="")
		for j in range(NUMBER_CANDIDATES):
			if cluster.labels_[j]==i:
				print(" "+id_to_candidates_pretty[j], end="")
		print()

	print("\nHashes:")
	for i in range(nbclusters):
		print("Class {}:".format(i), end="")
		for j in range(NUMBER_CANDIDATES):
			if cluster_hash.labels_[j]==i:
				print(" "+id_to_candidates_pretty[j], end="")
		print()


if __name__=="__main__":
	#collect_data()
	main()
